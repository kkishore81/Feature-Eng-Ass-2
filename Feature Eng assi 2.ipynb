{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2f67ed-9573-49f3-b679-b620a693d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "#application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f7d798-7d95-496f-b15e-229cf71d28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max scaling is a data preprocessing technique that transforms the values of each feature in a dataset to a range of [0, 1]. \n",
    "#This is done by subtracting the minimum value of each feature from all of its values and then dividing by the difference between the maximum and minimum values.\n",
    "#Min-Max scaling is used to normalize the data so that all features have a similar scale, which can improve the performance of machine learning algorithms.\n",
    "\n",
    "#For example, suppose we have a dataset with two features: height and weight.\n",
    "#The height values in the dataset range from 150 to 200 cm, while the weight values range from 50 to 100 kg.\n",
    "#Min-Max scaling would transform these values to a range of [0, 1] by subtracting the minimum value of each feature from all of its values and then dividing by the difference\n",
    "#between the maximum and minimum values. So, the height values would all be between 0 and 0.5, and the weight values would all be between 0 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fd1b75-c382-4046-99e1-41fb3043fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b88574-7937-4c36-ad3d-a28c497a06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Unit Vector technique in feature scaling involves transforming each feature to have a magnitude of 1 while preserving the direction of the original data points.\n",
    "#This is achieved by dividing each data point by its magnitude.\n",
    "\n",
    "#For example, suppose we have a dataset with two features: height and weight. The height values in the dataset range from 150 to 200 cm, \n",
    "#while the weight values range from 50 to 100 kg. Unit Vector scaling would transform these values by dividing each feature vector by its Euclidean length. \n",
    "#So, the height vector would be divided by its length of 15, and the weight vector would be divided by its length of 10. This would result in both feature vectors having a length of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e53ef-652f-4ef3-b2d1-80857826c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "#example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e223d4-9ba7-433c-846f-167ea1a1f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms a high-dimensional dataset into a lower-dimensional dataset while retaining \n",
    "#as much information as possible. This is done by finding the directions of greatest variance in the data and projecting the data onto these directions.\n",
    "\n",
    "#For example, suppose we have a dataset with 10 features, but we only care about the first two principal components. \n",
    "#PCA would transform the data into a 2-dimensional dataset by finding the directions of greatest variance in the data and projecting the data onto these directions. \n",
    "#This would result in a new dataset with only 2 features, but the first two principal components would capture most of the information in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dee32c-5943-4283-b70e-ebd33407789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "#Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690388d-6cc1-434e-9193-ec57672204e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (Principal Component Analysis) is a dimensionality reduction technique, while feature extraction is a process of selecting a subset of features from a \n",
    "#dataset that are most relevant to the problem at hand. PCA can be used for feature extraction by finding the principal components of the data, \n",
    "#which are the directions of greatest variance in the data. \n",
    "#The principal components can then be used to represent the data in a lower-dimensional space, while still retaining most of the information in the original data.\n",
    "\n",
    "#For example  PCA can be used for feature extraction in a real-world application. Suppose we have a dataset of customer transactions.\n",
    "#We want to use this dataset to build a model that predicts whether a customer will churn (cancel their subscription). PCA can be used to \n",
    "#find the principal components of the data, and then we can select the principal components \n",
    "#that have the strongest correlation with the churn label. This would result in a smaller subset of features that are still very informative \n",
    "#about whether a customer is likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf469ef-23e2-4412-9757-969acb0fc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "#contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "#preprocess the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f3a77d-2dce-4a97-8414-b2ec3d7f7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I would use Min-Max scaling to preprocess the data by subtracting the minimum value of each feature from all of its values and then dividing by the difference \n",
    "#between the maximum and minimum values.This would ensure that all features have a similar scale, which can improve the performance of the recommendation system.\n",
    "\n",
    "#For example, if the price of the cheapest food item in the dataset is Rs 10 and the price of the most expensive food item is Rs 100, then Min-Max scaling would\n",
    "#transform the price feature to a range of [0, 1]. This would make it easier for the recommendation system to compare the prices of different food items.\n",
    "\n",
    "#The same process would be applied to the rating and delivery time features. This would ensure that all three features have a similar scale, which can help\n",
    "#the recommendation system to make more accurate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9fc4f0-04de-45c8-814d-6de1d90af0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "#features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "#dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "625a8338-f8c4-4f01-af5e-f6f8bbaee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I would use PCA to reduce the dimensionality of the dataset by finding the principal components of the data. \n",
    "#The principal components are the directions of greatest variance in the data. \n",
    "#By projecting the data onto these directions, I can reduce the dimensionality of the dataset while still retaining most of the information in the original data.\n",
    "\n",
    "#For example, if the dataset contains 100 features, I could use PCA to reduce the dimensionality to 10 or 20 features.\n",
    "#This would make it easier to train a model to predict stock prices, as the model would have to deal with fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d8686e-fad2-4c48-8469-e66c48eba55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b79193ce-6320-4d90-af57-9c1f5071cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "# Reshape the data into a column vector and then apply Min-Max scaling\n",
    "scaled_data = min_max.fit_transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74308292-c980-41c6-8d9d-641fce849e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "#Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959731f-589b-428e-9a3f-f18c1a416da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d2e181d-7dac-4708-8a42-33a9d3658c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height  weight  age  gender  blood pressure\n",
      "0     170      65   23       0             120\n",
      "1     180      70   25       1             130\n",
      "2     160      60   21       0             110\n",
      "3     190      80   27       1             140\n",
      "4     175      68   24       0             125\n",
      "(5, 4)\n",
      "[[-1.26381517e+00 -3.33139420e-01 -6.38471170e-02  2.24721638e-17]\n",
      " [ 1.27748549e+00  7.89708122e-01 -1.97737035e-01  9.47862762e-18]\n",
      " [-2.98374459e+00  4.09295002e-01  1.82910655e-01  6.87992039e-18]\n",
      " [ 3.33969656e+00 -1.42527003e-01  2.01318200e-01  1.12110991e-17]\n",
      " [-3.69622293e-01 -7.23336701e-01 -1.22644702e-01  1.68250591e-18]]\n",
      "[9.36884938e-01 5.78269248e-02 5.28813751e-03 3.08278161e-35]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a sample dataset with 5 features\n",
    "X = np.array([[170, 65, 23, 0, 120], [180, 70, 25, 1, 130], [160, 60, 21, 0, 110], [190, 80, 27, 1, 140], [175, 68, 24, 0, 125]])\n",
    "df = pd.DataFrame(X, columns=['height', 'weight', 'age', 'gender', 'blood pressure'])\n",
    "print(df)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Perform feature extraction using PCA with n_components=4\n",
    "pca = decomposition.PCA(n_components=4)\n",
    "X_std_pca = pca.fit_transform(X_std)\n",
    "print(X_std_pca.shape)\n",
    "print(X_std_pca)\n",
    "\n",
    "# Print the explained variance ratio of each component\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e21bcc-9428-4045-bd9c-2413a84b689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The variance of each component is as follows:\n",
    "\n",
    "Component 1: 0.936884938\n",
    "Component 2: 0.0578269248\n",
    "Component 3: 0.00528813751\n",
    "Component 4: close to Zero\n",
    "\n",
    "I chose to retain the first three principal components because they capture 95.8% of the variance in the data. \n",
    "This means that the first three principal components contain most of the information in the original data.\n",
    "Retaining more principal components would not significantly improve the representation of the data,\n",
    "but it would also increase the computational complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
